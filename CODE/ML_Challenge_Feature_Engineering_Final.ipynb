{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "LLS9fPQMlTVu"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        },
        "id": "MgGxMehIlcNe",
        "outputId": "a2838b35-3d68-4c46-e205-54778dfd7286"
      },
      "outputs": [],
      "source": [
        "train = pd.read_parquet('train.parquet')\n",
        "test = pd.read_parquet('test.parquet')\n",
        "train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ErkgvpxCS4Da"
      },
      "source": [
        "# Drop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LcfVIoFcS5qR"
      },
      "outputs": [],
      "source": [
        "train = train.drop(['l_feat_16'],axis=1)\n",
        "test = test.drop(['l_feat_16'],axis=1)\n",
        "\n",
        "train = train.drop(['l_feat_17'],axis=1)\n",
        "test = test.drop(['l_feat_17'],axis=1)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5dTu4ZSHDSB9"
      },
      "source": [
        "# Target Encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bo9opd45DTRp"
      },
      "outputs": [],
      "source": [
        "def add_target_encoding(train, test, target_col, feature_combinations):\n",
        "\n",
        "    global_mean = train[target_col].mean()\n",
        "\n",
        "    for combo in feature_combinations:\n",
        "        combo_name = '+'.join(combo)\n",
        "        te_col = f'TE_{combo_name}'\n",
        "\n",
        "        group_mean = (\n",
        "            train\n",
        "            .groupby(list(combo))[target_col]\n",
        "            .mean()\n",
        "            .reset_index()\n",
        "            .rename(columns={target_col: te_col})\n",
        "        )\n",
        "\n",
        "        train = train.merge(group_mean, on=list(combo), how='left')\n",
        "        test = test.merge(group_mean, on=list(combo), how='left')\n",
        "\n",
        "        test[te_col] = test[te_col].fillna(global_mean)\n",
        "\n",
        "    return train, test\n",
        "\n",
        "feature_combinations = [\n",
        "    ('age_group', 'gender'),\n",
        "    ('age_group', 'hour'),\n",
        "    ('hour',),\n",
        "    ('inventory_id',),\n",
        "]\n",
        "\n",
        "train, test = add_target_encoding(train, test, target_col='clicked', feature_combinations=feature_combinations)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OTx_SNfJere6"
      },
      "source": [
        "# seq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wqsPBgxmbaWe",
        "outputId": "2355aa9d-08e6-43c5-a75f-9a6614c7a579"
      },
      "outputs": [],
      "source": [
        "\n",
        "####################################\n",
        "### 시퀀스 길이 자체 → 유저의 \"활성도\"를 의미\n",
        "####################################\n",
        "train['user_activity'] = train['seq'].str.count(',')\n",
        "test['user_activity'] = test['seq'].str.count(',')\n",
        "\n",
        "####################################\n",
        "### 최근 행동의 중요성 반영 + 과거 긴 히스토리의 노이즈 제거\n",
        "####################################\n",
        "def extract_last_seq_features(df, col='seq'):\n",
        "    \"\"\"\n",
        "    df['seq']에서 마지막 3개의 숫자를 추출하여\n",
        "    seq_last, seq_last_1, seq_last_2 컬럼을 생성해 df에 붙임\n",
        "    \"\"\"\n",
        "    last_values = []\n",
        "    last_1_values = []\n",
        "    last_2_values = []\n",
        "\n",
        "    for seq in tqdm(df[col], desc=\"Extracting last seq tokens\"):\n",
        "        if pd.isna(seq) or seq.strip() == \"\":\n",
        "            tokens = []\n",
        "        else:\n",
        "            tokens = seq.strip().split(',')\n",
        "\n",
        "        # 마지막 값들 추출 (없으면 np.nan)\n",
        "        last_values.append(int(tokens[-1]) if len(tokens) >= 1 else np.nan)\n",
        "        last_1_values.append(int(tokens[-2]) if len(tokens) >= 2 else np.nan)\n",
        "        last_2_values.append(int(tokens[-3]) if len(tokens) >= 3 else np.nan)\n",
        "    df['seq_last'] = last_values\n",
        "    df['seq_last_1'] = last_1_values\n",
        "    df['seq_last_2'] = last_2_values\n",
        "    return df\n",
        "\n",
        "train = extract_last_seq_features(train)\n",
        "test = extract_last_seq_features(test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AqrXtGfandrU"
      },
      "source": [
        "# inventory_id"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wrBC8A5bEuzs"
      },
      "source": [
        "# hour"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-geoOk6HEzZe"
      },
      "outputs": [],
      "source": [
        "def add_hour_sin_cos(df, hour_col='hour', period=24):\n",
        "\n",
        "    df[f'{hour_col}_sin'] = np.sin(2 * np.pi * df[hour_col] / period)\n",
        "    df[f'{hour_col}_cos'] = np.cos(2 * np.pi * df[hour_col] / period)\n",
        "    return df\n",
        "\n",
        "train['hour'] = train['hour'].astype(str).str.zfill(2).astype(int)\n",
        "test['hour'] = test['hour'].astype(str).str.zfill(2).astype(int)\n",
        "\n",
        "train = add_hour_sin_cos(train, hour_col='hour')\n",
        "test = add_hour_sin_cos(test, hour_col='hour')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IOTVk555FDG4"
      },
      "source": [
        "# day_of_week"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TPVD-_JlFELJ"
      },
      "outputs": [],
      "source": [
        "def add_day_sin_cos(df, day_col='day_of_week', period=7):\n",
        "\n",
        "    df[f'{day_col}_sin'] = np.sin(2 * np.pi * df[day_col] / period)\n",
        "    df[f'{day_col}_cos'] = np.cos(2 * np.pi * df[day_col] / period)\n",
        "    return df\n",
        "\n",
        "train['day_of_week'] = train['day_of_week'].astype(str).str.zfill(2).astype(int)\n",
        "test['day_of_week'] = test['day_of_week'].astype(str).str.zfill(2).astype(int)\n",
        "\n",
        "train = add_day_sin_cos(train, day_col='day_of_week')\n",
        "test = add_day_sin_cos(test, day_col='day_of_week')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NMV3QGakGZFR"
      },
      "source": [
        "# Combination"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dy_-C8PqGaPv"
      },
      "outputs": [],
      "source": [
        "####################################\n",
        "### gender + age_group\n",
        "####################################\n",
        "train['age_gender'] = train['age_group'] + \"_\" + train['gender']\n",
        "test['age_gender'] = test['age_group'] + \"_\" + test['gender']\n",
        "\n",
        "\n",
        "####################################\n",
        "### gender + age_group은 20대 남자, 30대 남자는 유사하다 처럼 이런 스무스?한 정보를 표현할 수 없음. 따라서 피처엔지니어링\n",
        "####################################\n",
        "manual_groups = {\n",
        "    ('1.0_1.0', '1.0_2.0'): 'grp_A',\n",
        "    ('8.0_1.0', '8.0_2.0'): 'grp_B',\n",
        "    ('3.0_1.0', '4.0_1.0', '5.0_1.0', '5.0_2.0'): 'grp_C',\n",
        "    ('3.0_2.0', '4.0_2.0'): 'grp_D'\n",
        "}\n",
        "age_gender_custom_map = {}\n",
        "for group_vals, group_name in manual_groups.items():\n",
        "    for val in group_vals:\n",
        "        age_gender_custom_map[val] = group_name\n",
        "def map_age_gender_group(val):\n",
        "    return age_gender_custom_map.get(val, val)  # 지정된 것만 그룹 이름, 나머지는 원본 유지\n",
        "# 새 피처: age_gender_smiliarity\n",
        "train['age_gender_smiliarity'] = train['age_gender'].apply(map_age_gender_group)\n",
        "test['age_gender_smiliarity'] = test['age_gender'].apply(map_age_gender_group)\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.special import betaln, digamma\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.special import betaln, digamma\n",
        "\n",
        "RARE_THRESHOLD = 1500\n",
        "\n",
        "# 2️⃣ 설정\n",
        "GROUP_SETS = [\n",
        "    [\"inventory_id\"],\n",
        "]\n",
        "\n",
        "PRIOR_CTR = train[\"clicked\"].mean()\n",
        "PRIOR_N = 200\n",
        "PRIOR_A = PRIOR_CTR * PRIOR_N\n",
        "PRIOR_B = (1 - PRIOR_CTR) * PRIOR_N\n",
        "\n",
        "# 3️⃣ 베이지안 피처 생성\n",
        "for _i, _cols in enumerate(GROUP_SETS):\n",
        "    _suf = \"x\".join(_cols)\n",
        "\n",
        "    agg_tr = (\n",
        "        train.groupby(_cols)[\"clicked\"]\n",
        "        .agg(imps=\"count\", clicks=\"sum\")\n",
        "        .reset_index()\n",
        "    )\n",
        "\n",
        "    train = train.merge(agg_tr, on=_cols, how=\"left\", suffixes=(\"\", f\"__agg_{_suf}\"))\n",
        "\n",
        "    a_tr = PRIOR_A + train[\"clicks\"].astype(float)\n",
        "    b_tr = PRIOR_B + (train[\"imps\"].astype(float) - train[\"clicks\"].astype(float))\n",
        "    ab_tr = a_tr + b_tr\n",
        "\n",
        "    train[f\"unc_{_suf}_post_mean\"] = a_tr / (ab_tr + 1e-12)\n",
        "    train[f\"unc_{_suf}_post_var\"] = (a_tr * b_tr) / ((ab_tr**2) * (ab_tr + 1.0) + 1e-12)\n",
        "    lnB_tr = betaln(a_tr, b_tr)\n",
        "    train[f\"unc_{_suf}_post_entropy\"] = (\n",
        "        lnB_tr\n",
        "        - (a_tr - 1.0) * digamma(a_tr)\n",
        "        - (b_tr - 1.0) * digamma(b_tr)\n",
        "        + (ab_tr - 2.0) * digamma(ab_tr)\n",
        "    )\n",
        "    train[f\"unc_{_suf}_eff_n\"] = ab_tr\n",
        "    train.drop(columns=[\"imps\", \"clicks\"], inplace=True)\n",
        "\n",
        "    # === test 병합 ===\n",
        "    a_te = PRIOR_A + agg_tr[\"clicks\"].astype(float).to_numpy()\n",
        "    b_te = PRIOR_B + (agg_tr[\"imps\"].astype(float).to_numpy() - agg_tr[\"clicks\"].astype(float).to_numpy())\n",
        "    ab_te = a_te + b_te\n",
        "\n",
        "    agg_tr_features = agg_tr.copy()\n",
        "    agg_tr_features[f\"unc_{_suf}_post_mean\"] = a_te / (ab_te + 1e-12)\n",
        "    agg_tr_features[f\"unc_{_suf}_post_var\"] = (a_te * b_te) / ((ab_te**2) * (ab_te + 1.0) + 1e-12)\n",
        "    lnB_te = betaln(a_te, b_te)\n",
        "    agg_tr_features[f\"unc_{_suf}_post_entropy\"] = (\n",
        "        lnB_te\n",
        "        - (a_te - 1.0) * digamma(a_te)\n",
        "        - (b_te - 1.0) * digamma(b_te)\n",
        "        + (ab_te - 2.0) * digamma(ab_te)\n",
        "    )\n",
        "    agg_tr_features[f\"unc_{_suf}_eff_n\"] = ab_te\n",
        "\n",
        "    test = test.merge(\n",
        "        agg_tr_features[_cols + [\n",
        "            f\"unc_{_suf}_post_mean\",\n",
        "            f\"unc_{_suf}_post_var\",\n",
        "            f\"unc_{_suf}_post_entropy\",\n",
        "            f\"unc_{_suf}_eff_n\"\n",
        "        ]],\n",
        "        on=_cols, how=\"left\"\n",
        "    )\n",
        "\n",
        "    prior_ab = PRIOR_A + PRIOR_B\n",
        "    prior_mean = PRIOR_A / (prior_ab + 1e-12)\n",
        "    prior_var = (PRIOR_A * PRIOR_B) / ((prior_ab**2) * (prior_ab + 1.0) + 1e-12)\n",
        "    prior_ent = (\n",
        "        betaln(PRIOR_A, PRIOR_B)\n",
        "        - (PRIOR_A - 1.0) * digamma(PRIOR_A)\n",
        "        - (PRIOR_B - 1.0) * digamma(PRIOR_B)\n",
        "        + (prior_ab - 2.0) * digamma(prior_ab)\n",
        "    )\n",
        "\n",
        "    # 무조건 prior 값만 사용 (리키지 없음)\n",
        "    fallback_val = {\n",
        "        f\"unc_{_suf}_post_mean\": prior_mean,\n",
        "        f\"unc_{_suf}_post_var\": prior_var,\n",
        "        f\"unc_{_suf}_post_entropy\": prior_ent,\n",
        "        f\"unc_{_suf}_eff_n\": prior_ab\n",
        "    }\n",
        "\n",
        "    # NaN 채우기 (머지 실패한 test 행들)\n",
        "    for key in fallback_val:\n",
        "        test[key] = test[key].fillna(fallback_val[key])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SH_ovQOY-6i-"
      },
      "outputs": [],
      "source": [
        "train.drop(['seq'],axis=1,inplace=True)\n",
        "test.drop(['seq'],axis=1,inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PFSSc3Nh2MoZ"
      },
      "outputs": [],
      "source": [
        "test.drop(['ID'],axis=1,inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0pcoWV7Oj-Mo"
      },
      "outputs": [],
      "source": [
        "for col in train.select_dtypes(include=[\"int64\"]).columns:\n",
        "    train[col] = train[col].astype(np.int32)\n",
        "for col in test.select_dtypes(include=[\"int64\"]).columns:\n",
        "    test[col] = test[col].astype(np.int32)\n",
        "for col in train.select_dtypes(include=[\"float64\"]).columns:\n",
        "    train[col] = train[col].astype(np.float32)\n",
        "for col in test.select_dtypes(include=[\"float64\"]).columns:\n",
        "    test[col] = test[col].astype(np.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for col in train.columns:\n",
        "    if(col == 'clicked'):\n",
        "      continue\n",
        "    if train[col].isnull().sum() > 0:\n",
        "        if pd.api.types.is_categorical_dtype(train[col]):\n",
        "            if 'Null' not in train[col].cat.categories:\n",
        "                train[col] = train[col].cat.add_categories('Null')\n",
        "                test[col] = test[col].cat.add_categories('Null')\n",
        "            train[col] = train[col].fillna('Null')\n",
        "            test[col] = test[col].fillna('Null')\n",
        "\n",
        "        elif train[col].dtype == 'object':\n",
        "            train[col] = train[col].fillna('Null')\n",
        "            test[col] = test[col].fillna('Null')\n",
        "\n",
        "        elif np.issubdtype(train[col].dtype, np.number):\n",
        "            train[col] = train[col].fillna(-99999)\n",
        "            test[col] = test[col].fillna(-99999)\n",
        "for col in train.select_dtypes(include=[\"object\"]).columns:\n",
        "    train[col] = train[col].astype(\"category\")\n",
        "for col in test.select_dtypes(include=[\"object\"]).columns:\n",
        "    test[col] = test[col].astype(\"category\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eNoAKc3S2aVU"
      },
      "outputs": [],
      "source": [
        "train.to_parquet('train_preprocess_Final.parquet')\n",
        "test.to_parquet('test_preprocess_Final.parquet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kWveUju1aHke"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
